# From Individual Claims to Systemic Patterns: Extracting a Dataset from Police Misconduct Litigation

**Elijah Appelson** â€” Stanford University â€” [appelson@stanford.edu](mailto:appelson@stanford.edu)  
**Zooey Carter Wilkinson** â€” Stanford University â€” [zooeycw@stanford.edu](mailto:zooeycw@stanford.edu)

---

This repository contains the full data-processing, extraction, and analysis pipeline associated with the paper **"From Individual Claims to Systemic Patterns: Extracting a Dataset from Police Misconduct Litigation."**

The paper investigates whether modern large language modelsâ€”specifically a zero-shot GPT-4o-Mini pipelineâ€”can convert federal police-misconduct complaints into structured data identifying:

* Law-enforcement agencies
* Individual officers
* Plaintiffs and demographic attributes
* Misconduct allegations
* Locations of incidents
* Causes of action

This repository provides the code, processing workflow, and data schema used in the study.

> **Note:** Raw complaint PDFs are not included due to licensing restrictions, but complete sample inputs and outputs are provided.

## ğŸ§© Project Overview

Civil-rights complaints contain rich narrative accounts of interactions with law enforcement, yet no public dataset systematically identifies which agencies are sued or what harms are alleged. The accompanying paper evaluates LLM extraction performance on **13,657 complaints** across **7,552 terminated â€œpolice actionâ€ cases (2015â€“2025)** in Lex Machinaâ€™s Ninth Circuit dataset.

This repository includes:

* OCR and text-preparation scripts
* Tokenization pipeline
* Zero-shot GPT-4o-Mini extraction workflow
* Cleaning and normalization code
* Clustering to consolidate agency and officer variants
* Validation and accuracy checks
* Analysis scripts used in the paper

It is meant to make the study reproducible, transparent, and extensible.

## ğŸ“ Repository Structure

```text
litigation_2_data/
â”œâ”€â”€ 1_loading_data/        # Raw data ingestion and initial formatting
â”œâ”€â”€ 2_tokenization/        # Python tokenization into structured JSON
â”œâ”€â”€ 3_extraction/          # LLM extraction scripts + extraction prompt
â”œâ”€â”€ 4_aggregation/         # Merge, clean, and standardize extracted fields
â”œâ”€â”€ 5_validation/          # Evaluation metrics and error checks
â”œâ”€â”€ 6_analysis/            # Scripts used to generate analytical outputs
â”œâ”€â”€ annotate_app/          # Shiny app for human annotation & inspection
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data/          # (No PDFs included) Metadata + OCR text
â”‚   â”œâ”€â”€ extract/           # Model-generated extracted text
â”‚   â”œâ”€â”€ clean_data/        # Cleaned CSVs used in the paper
â”‚   â”œâ”€â”€ sample_data/       # Small demo dataset for reproducibility
â”‚   â””â”€â”€ tokenized_json/    # Structured tokenization outputs
â””â”€â”€ README.md
```
